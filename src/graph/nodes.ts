import { chatModel } from "../models/openai";
import { restaurantSearch } from "../tools/serpTool";
import { State } from "../graph/state";
import { HumanMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { RESTAURANT_PROMPT_TEMPLATE } from "../prompts/restaurantPrompt";
import { loadDocuments } from "../rag/loader";
import { splitDocuments } from "../rag/splitter";
import { createVectorStore } from "../rag/store";
import { createRAGAgent } from "../agents/ragAgent";

export async function llmNode(state: State): Promise<State> {
  const llmWithTools = chatModel.bindTools([restaurantSearch]);

  // If there are no messages or the array is empty, create the initial message
  const isFirstCall = !state.messages || state.messages.length === 0;
  const messages = isFirstCall
    ? [
        new HumanMessage({
          content: RESTAURANT_PROMPT_TEMPLATE(state.userQuery),
        }),
      ]
    : state.messages;

  console.log("\nğŸ”µ [llmNode] ========================================");
  console.log(`ğŸ”µ [llmNode] Is first call: ${isFirstCall}`);
  console.log(`ğŸ”µ [llmNode] Message history: ${messages.length} messages`);

  if (!isFirstCall) {
    console.log(`ğŸ”µ [llmNode] Last message received:`, {
      type: messages[messages.length - 1]?.constructor.name,
      isToolMessage: messages[messages.length - 1] instanceof ToolMessage,
    });
  }

  const response = await llmWithTools.invoke(messages);

  const hasToolCalls = response.tool_calls && response.tool_calls.length > 0;
  console.log(`ğŸ”µ [llmNode] LLM response received`);
  console.log(`ğŸ”µ [llmNode] Has tool_calls?: ${hasToolCalls}`);

  if (hasToolCalls && response.tool_calls) {
    console.log(
      `ğŸ”µ [llmNode] Number of tool_calls: ${response.tool_calls.length}`
    );
    response.tool_calls.forEach((tc, idx) => {
      console.log(`ğŸ”µ [llmNode]   Tool call ${idx + 1}: ${tc.name}`);
      console.log(`ğŸ”µ [llmNode]   Args:`, JSON.stringify(tc.args, null, 2));
    });
    console.log(
      `ğŸ”µ [llmNode] â†’ LLM wants to execute tools to get more information`
    );
  } else {
    const responsePreview =
      typeof response.content === "string"
        ? response.content.substring(0, 150)
        : JSON.stringify(response.content).substring(0, 150);
    console.log(`ğŸ”µ [llmNode] LLM response (preview): ${responsePreview}...`);
    console.log(
      `ğŸ”µ [llmNode] â†’ LLM has enough information, generating final response`
    );
  }

  // Add LLM to history
  const updatedMessages = [...messages, new AIMessage(response)];

  return {
    ...state,
    messages: updatedMessages,
  };
}

export async function toolsNode(state: State): Promise<State> {
  const messages = state.messages || [];
  const lastMessage = messages[messages.length - 1];

  console.log("\nğŸŸ¢ [toolsNode] ========================================");
  console.log(`ğŸŸ¢ [toolsNode] Executing tools...`);

  // Verify that the last message is from the LLM and has tool_calls
  if (
    !(lastMessage instanceof AIMessage) ||
    !lastMessage.tool_calls ||
    lastMessage.tool_calls.length === 0
  ) {
    console.log(`ğŸŸ¢ [toolsNode] âš ï¸  No tool_calls to execute, returning state`);
    return state; // No tools to execute
  }

  console.log(
    `ğŸŸ¢ [toolsNode] Number of tools to execute: ${lastMessage.tool_calls.length}`
  );
  const toolMessages: ToolMessage[] = [];

  for (let idx = 0; idx < lastMessage.tool_calls.length; idx++) {
    const toolCall = lastMessage.tool_calls[idx];
    console.log(
      `\nğŸŸ¢ [toolsNode] --- Executing tool ${idx + 1}/${lastMessage.tool_calls.length} ---`
    );
    console.log(`ğŸŸ¢ [toolsNode] Name: ${toolCall.name}`);
    console.log(`ğŸŸ¢ [toolsNode] ID: ${toolCall.id}`);
    console.log(
      `ğŸŸ¢ [toolsNode] Arguments:`,
      JSON.stringify(toolCall.args, null, 2)
    );

    try {
      let toolResult: string;

      if (toolCall.name === "restaurantSearch" || toolCall.name === "search") {
        const searchInput =
          toolCall.args.input || toolCall.args.query || toolCall.args;
        console.log(`ğŸŸ¢ [toolsNode] â†’ Executing search with SerpAPI...`);
        console.log(`ğŸŸ¢ [toolsNode] Search query:`, searchInput);
        toolResult = await restaurantSearch.invoke(searchInput);
        const resultPreview =
          typeof toolResult === "string"
            ? toolResult.substring(0, 200)
            : JSON.stringify(toolResult).substring(0, 200);
        console.log(
          `ğŸŸ¢ [toolsNode] âœ… Result received (preview): ${resultPreview}...`
        );
        console.log(
          `ğŸŸ¢ [toolsNode] â†’ This result will be sent back to the LLM for processing`
        );
      } else {
        console.log(`ğŸŸ¢ [toolsNode] âš ï¸  Unknown tool: ${toolCall.name}`);
        toolResult = `Tool ${toolCall.name} not found`;
      }

      // Create tool message with the result
      toolMessages.push(
        new ToolMessage({
          content: toolResult,
          tool_call_id: toolCall.id || "",
        })
      );
    } catch (error) {
      console.error(`ğŸŸ¢ [toolsNode] âŒ Error executing tool:`, error);
      const errorMessage = `Error: ${error instanceof Error ? error.message : String(error)}`;
      toolMessages.push(
        new ToolMessage({
          content: errorMessage,
          tool_call_id: toolCall.id || "",
        })
      );
    }
  }

  console.log(`\nğŸŸ¢ [toolsNode] âœ… All tools executed`);
  console.log(
    `ğŸŸ¢ [toolsNode] â†’ Results will be added to history and LLM will be called again`
  );
  console.log(
    `ğŸŸ¢ [toolsNode] â†’ LLM will decide if it needs more information or can respond`
  );

  // Add results to history
  const updatedMessages = [...messages, ...toolMessages];

  return {
    ...state,
    messages: updatedMessages,
  };
}
export function shouldSearchInInternet(state: State): "internet" | "end" {
  console.log(`ğŸŸ¡ [shouldSearchInInternet] Checking RAG result...`);
  console.log(
    `ğŸŸ¡ [shouldSearchInInternet] cuisinePreferences: ${state.cuisinePreferences}`
  );

  if (state.cuisinePreferences === "false") {
    console.log(
      `ğŸŸ¡ [shouldSearchInInternet] â†’ Not found in RAG, searching on internet`
    );
    return "internet";
  } else {
    console.log(`ğŸŸ¡ [shouldSearchInInternet] â†’ Found in RAG, finalizing`);
    return "end";
  }
}
export function shouldExecuteTools(state: State): "tools" | "end" {
  const messages = state.messages || [];

  console.log(
    "\nğŸŸ¡ [shouldExecuteTools] ========================================"
  );
  console.log(
    `ğŸŸ¡ [shouldExecuteTools] Evaluating whether to continue with tools or finalize...`
  );
  console.log(
    `ğŸŸ¡ [shouldExecuteTools] Total messages in history: ${messages.length}`
  );

  // If there are no messages, we cannot continue (this shouldn't happen, but for safety)
  if (messages.length === 0) {
    console.log(`ğŸŸ¡ [shouldExecuteTools] âš ï¸  No messages, finalizing`);
    return "end";
  }

  const lastMessage = messages[messages.length - 1];
  console.log(`ğŸŸ¡ [shouldExecuteTools] Last message:`, {
    type: lastMessage.constructor.name,
    isAIMessage: lastMessage instanceof AIMessage,
    isToolMessage: lastMessage instanceof ToolMessage,
  });

  // If the last message is from the LLM and has tool_calls, we need to execute tools
  if (
    lastMessage instanceof AIMessage &&
    lastMessage.tool_calls &&
    lastMessage.tool_calls.length > 0
  ) {
    console.log(
      `ğŸŸ¡ [shouldExecuteTools] âœ… LLM wants to execute ${lastMessage.tool_calls.length} tool(s)`
    );
    console.log(
      `ğŸŸ¡ [shouldExecuteTools] â†’ Decision: GO TO TOOLS (execute tools)`
    );
    console.log(
      `ğŸŸ¡ [shouldExecuteTools] â†’ Reason: LLM needs more information before responding`
    );
    return "tools";
  }

  // If there are no tool_calls, the LLM has the final response
  if (lastMessage instanceof AIMessage) {
    console.log(`ğŸŸ¡ [shouldExecuteTools] âœ… LLM has no more tool_calls`);
    console.log(
      `ğŸŸ¡ [shouldExecuteTools] â†’ Decision: FINALIZE (LLM has the final response)`
    );
    console.log(
      `ğŸŸ¡ [shouldExecuteTools] â†’ Reason: LLM already has enough information to respond`
    );
  } else if (lastMessage instanceof ToolMessage) {
    console.log(`ğŸŸ¡ [shouldExecuteTools] âœ… Last message is a tool result`);
    console.log(
      `ğŸŸ¡ [shouldExecuteTools] â†’ Decision: FINALIZE (waiting for LLM response in next iteration)`
    );
    console.log(
      `ğŸŸ¡ [shouldExecuteTools] â†’ Note: This shouldn't happen, the LLM should have already responded`
    );
  }

  return "end";
}

export async function ragNode(state: State): Promise<State> {
  console.log("\nğŸŸ£ [ragNode] ========================================");
  console.log(`ğŸŸ£ [ragNode] Initializing RAG system...`);

  // Initialize RAG components
  const docs = await loadDocuments();
  const allSplits = await splitDocuments(docs);
  const vectorStore = await createVectorStore(allSplits);
  const agent = await createRAGAgent(vectorStore);

  console.log(`ğŸŸ£ [ragNode] RAG system initialized`);
  console.log(`ğŸŸ£ [ragNode] Processing query: ${state.userQuery}`);

  console.log(`\nğŸŸ£ [ragNode] ğŸ” Checking direct search in vector store...`);
  const directSearch = await vectorStore.similaritySearch(state.userQuery, 2);
  const hasDirectResults =
    directSearch.length > 0 &&
    directSearch.some(
      (doc) =>
        doc.pageContent.toLowerCase().includes("restaurant") ||
        doc.pageContent.toLowerCase().includes("tandil")
    );
  console.log(
    `ğŸŸ£ [ragNode] ğŸ“Š Direct search results: ${directSearch.length} documents found`
  );
  console.log(
    `ğŸŸ£ [ragNode] ${hasDirectResults ? "âœ…" : "âŒ"} Are there relevant results?: ${hasDirectResults}`
  );
  if (directSearch.length > 0) {
    console.log(
      `ğŸŸ£ [ragNode] ğŸ“„ First result (preview): ${directSearch[0].pageContent.substring(0, 150)}...`
    );
  }

  // Execute RAG agent with user query
  const agentInputs = {
    messages: [{ role: "user", content: state.userQuery }],
  };

  const stream = await agent.stream(agentInputs, {
    streamMode: "values",
  });

  let ragResponse = "";
  for await (const step of stream) {
    const lastMessage = step.messages[step.messages.length - 1];
    const messageType = lastMessage.getType();
    const messageContent =
      typeof lastMessage.content === "string"
        ? lastMessage.content
        : JSON.stringify(lastMessage.content);
    // Capture the final response
    if (messageType === "ai" || messageType === "assistant") {
      ragResponse = messageContent;
    }
  }

  console.log(`\nğŸŸ£ [ragNode] âœ… RAG processing completed`);
  console.log(`ğŸŸ£ [ragNode] Response: ${ragResponse.substring(0, 200)}...`);

  // Option 1: Analysis of RAG response
  console.log(`\nğŸŸ£ [ragNode] ğŸ” Analyzing RAG response...`);
  const responseLower = ragResponse.toLowerCase();
  const hasNegativeIndicators =
    responseLower.includes("false") ||
    responseLower.includes("no encontrÃ©") ||
    responseLower.includes("no encontre") ||
    responseLower.includes("no hay") ||
    responseLower.includes("no existe") ||
    responseLower.trim().length < 10; // Very short responses are likely negative

  const hasPositiveIndicators =
    responseLower.includes("restaurant") ||
    responseLower.includes("restaurante") ||
    responseLower.includes("tandil") ||
    (ragResponse.length > 50 && !hasNegativeIndicators); // Long responses without negative indicators

  const foundRestaurant = !hasNegativeIndicators && hasPositiveIndicators;

  console.log(
    `ğŸŸ£ [ragNode] ğŸ“Š Negative indicators found: ${hasNegativeIndicators}`
  );
  console.log(
    `ğŸŸ£ [ragNode] ğŸ“Š Positive indicators found: ${hasPositiveIndicators}`
  );
  console.log(
    `ğŸŸ£ [ragNode] ${foundRestaurant ? "âœ…" : "âŒ"} Restaurant found?: ${foundRestaurant}`
  );

  // Final summary
  const finalVerification = hasDirectResults && foundRestaurant;
  console.log(
    `\nğŸŸ£ [ragNode] ğŸ¯ FINAL VERIFICATION: ${finalVerification ? "âœ… RESTAURANT FOUND" : "âŒ RESTAURANT NOT FOUND"}`
  );
  console.log(
    `ğŸŸ£ [ragNode]   - Direct search: ${hasDirectResults ? "âœ…" : "âŒ"}`
  );
  console.log(
    `ğŸŸ£ [ragNode]   - Response analysis: ${foundRestaurant ? "âœ…" : "âŒ"}`
  );

  // If restaurant not found, return "false" for later evaluation
  const finalResponse = finalVerification
    ? ragResponse || "No response from RAG"
    : "false";

  console.log(
    `ğŸŸ£ [ragNode] ğŸ“¤ Final response: ${finalResponse === "false" ? "false (not found)" : finalResponse.substring(0, 100) + "..."}`
  );

  return {
    ...state,
    cuisinePreferences: finalResponse,
  };
}

export async function finalizeNode(state: State): Promise<State> {
  const messages = state.messages || [];

  console.log("\nğŸ”´ [finalizeNode] ========================================");
  console.log(`ğŸ”´ [finalizeNode] Extracting final response...`);
  console.log(`ğŸ”´ [finalizeNode] Total messages processed: ${messages.length}`);

  // If there's already a RAG response (and it's not "false"), use it directly
  if (
    state.cuisinePreferences &&
    state.cuisinePreferences !== "false" &&
    state.cuisinePreferences !== "No response from RAG"
  ) {
    console.log(`ğŸ”´ [finalizeNode] âœ… Using RAG response`);
    console.log(
      `ğŸ”´ [finalizeNode] Final response (preview): ${state.cuisinePreferences.substring(0, 200)}...`
    );
    console.log(`ğŸ”´ [finalizeNode] âœ… Process completed`);

    return {
      ...state,
      cuisinePreferences: state.cuisinePreferences,
    };
  }

  // If there's no RAG response, search in messages (normal LLM flow)
  let finalResponse = "";

  for (let i = messages.length - 1; i >= 0; i--) {
    const message = messages[i];
    if (
      message instanceof AIMessage &&
      (!message.tool_calls || message.tool_calls.length === 0)
    ) {
      finalResponse =
        typeof message.content === "string"
          ? message.content
          : JSON.stringify(message.content);
      console.log(
        `ğŸ”´ [finalizeNode] âœ… Final response found in message ${i + 1}/${messages.length}`
      );
      break;
    }
  }

  if (!finalResponse) {
    console.log(
      `ğŸ”´ [finalizeNode] âš ï¸  Final response not found, using last message`
    );
    if (messages.length > 0) {
      const lastMessage = messages[messages.length - 1];
      finalResponse =
        typeof lastMessage.content === "string"
          ? lastMessage.content
          : JSON.stringify(lastMessage.content);
    }
  }

  const responsePreview = finalResponse.substring(0, 200);
  console.log(
    `ğŸ”´ [finalizeNode] Final response (preview): ${responsePreview}...`
  );
  console.log(`ğŸ”´ [finalizeNode] âœ… Process completed`);

  return {
    ...state,
    cuisinePreferences: finalResponse || "No response generated",
  };
}
